{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Statistical Hypothesis Testing           \n",
    "Example: Flipping a Coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.stats' has no attribute 'binom_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m successes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(flips)\n\u001b[1;32m     13\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(flips)\n\u001b[0;32m---> 14\u001b[0m p_value \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinom_test\u001b[49m(successes, n, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreater\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of successes (heads): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuccesses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.stats' has no attribute 'binom_test'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Flipping a fair coin 100 times\n",
    "np.random.seed(0)\n",
    "flips = np.random.binomial(n=1, p=0.5, size=100)\n",
    "\n",
    "# Null hypothesis: the coin is fair (p=0.5)\n",
    "# Alternative hypothesis: the coin is not fair (p != 0.5)\n",
    "\n",
    "# Performing a binomial test\n",
    "successes = np.sum(flips)\n",
    "n = len(flips)\n",
    "p_value = stats.binom_test(successes, n, p=0.5, alternative='greater')\n",
    "\n",
    "print(f\"Number of successes (heads): {successes}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: the coin is not fair.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: the coin is fair.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. p-Values             \n",
    "Example: One-sample t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data: weights of 10 apples\n",
    "np.random.seed(0)\n",
    "weights = np.random.normal(loc=150, scale=10, size=10)\n",
    "\n",
    "# Null hypothesis: mean weight = 150\n",
    "# Alternative hypothesis: mean weight != 150\n",
    "\n",
    "# Performing a one-sample t-test\n",
    "mean_weight = 150\n",
    "t_stat, p_value = stats.ttest_1samp(weights, mean_weight)\n",
    "\n",
    "print(f\"Sample weights: {weights}\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: the mean weight is not 150.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: the mean weight is 150.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Confidence Intervals     \n",
    "Example: Confidence Interval for the Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data: weights of 10 apples\n",
    "np.random.seed(0)\n",
    "weights = np.random.normal(loc=150, scale=10, size=10)\n",
    "\n",
    "# Calculate the mean and standard error of the sample\n",
    "mean_weight = np.mean(weights)\n",
    "sem = stats.sem(weights)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_level = 0.95\n",
    "ci = stats.t.interval(confidence_level, len(weights) - 1, loc=mean_weight, scale=sem)\n",
    "\n",
    "print(f\"Sample weights: {weights}\")\n",
    "print(f\"Mean weight: {mean_weight:.2f}\")\n",
    "print(f\"95% Confidence interval: {ci}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. p-Hacking        \n",
    "Example: Multiple Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Generate random data for 20 tests\n",
    "np.random.seed(0)\n",
    "data = np.random.normal(loc=0, scale=1, size=(20, 100))\n",
    "\n",
    "# Perform t-tests for each test\n",
    "p_values = [stats.ttest_1samp(sample, 0).pvalue for sample in data]\n",
    "\n",
    "# Correct for multiple comparisons using Bonferroni correction\n",
    "alpha = 0.05\n",
    "corrected_alpha = alpha / len(p_values)\n",
    "significant_tests = [p < corrected_alpha for p in p_values]\n",
    "\n",
    "print(f\"p-values: {p_values}\")\n",
    "print(f\"Corrected alpha: {corrected_alpha:.4f}\")\n",
    "print(f\"Significant tests: {significant_tests}\")\n",
    "print(f\"Number of significant tests: {sum(significant_tests)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Example: Running an A/B Test     \n",
    "Example: A/B Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulated data for an A/B test\n",
    "np.random.seed(0)\n",
    "control = np.random.binomial(1, 0.4, 100)  # Control group with 40% success rate\n",
    "treatment = np.random.binomial(1, 0.5, 100)  # Treatment group with 50% success rate\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(control, treatment)\n",
    "\n",
    "print(f\"Control group mean: {np.mean(control):.2f}\")\n",
    "print(f\"Treatment group mean: {np.mean(treatment):.2f}\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: there is a significant difference between the groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: there is no significant difference between the groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Bayesian Inference       \n",
    "Example: Bayesian Updating for a Binomial Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "\n",
    "# Prior parameters\n",
    "alpha_prior = 2\n",
    "beta_prior = 2\n",
    "\n",
    "# Observed data: 6 heads and 4 tails\n",
    "successes = 6\n",
    "failures = 4\n",
    "\n",
    "# Posterior parameters\n",
    "alpha_post = alpha_prior + successes\n",
    "beta_post = beta_prior + failures\n",
    "\n",
    "# Plotting the prior and posterior distributions\n",
    "x = np.linspace(0, 1, 100)\n",
    "prior = beta(alpha_prior, beta_prior).pdf(x)\n",
    "posterior = beta(alpha_post, beta_post).pdf(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, prior, label=f'Prior: Beta({alpha_prior}, {beta_prior})')\n",
    "plt.plot(x, posterior, label=f'Posterior: Beta({alpha_post}, {beta_post})')\n",
    "plt.xlabel('Probability of Success')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Prior and Posterior Distributions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Gradient Descent     \n",
    "Example: Gradient Descent to Minimize a Quadratic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Quadratic function: f(x) = x^2\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Derivative of the quadratic function: f'(x) = 2x\n",
    "def f_prime(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Gradient descent parameters\n",
    "x0 = 10  # Initial guess\n",
    "learning_rate = 0.1\n",
    "iterations = 20\n",
    "\n",
    "# Performing gradient descent\n",
    "x = x0\n",
    "x_values = [x]\n",
    "\n",
    "for _ in range(iterations):\n",
    "    x -= learning_rate * f_prime(x)\n",
    "    x_values.append(x)\n",
    "\n",
    "# Plotting the function and the gradient descent steps\n",
    "x_range = np.linspace(-10, 10, 400)\n",
    "y_range = f(x_range)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, y_range, label='f(x) = x^2')\n",
    "plt.scatter(x_values, [f(x) for x in x_values], color='red')\n",
    "plt.plot(x_values, [f(x) for x in x_values], color='red', linestyle='--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent to Minimize f(x) = x^2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Minibatch and Stochastic Gradient Descent        \n",
    "Example: Stochastic Gradient Descent (SGD) for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data for linear regression\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Add bias term (intercept) to the feature matrix\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Initialize parameters\n",
    "theta = np.random.randn(2, 1)\n",
    "learning_rate = 0.1\n",
    "iterations = 50\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "for iteration in range(iterations):\n",
    "    for i in range(100):\n",
    "        random_index = np.random.randint(100)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        theta -= learning_rate * gradients\n",
    "\n",
    "# Plotting the data and the linear regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, X_b.dot(theta), color='red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Stochastic Gradient Descent for Linear Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Data Handling        \n",
    "Example: Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data file: 'data.txt'\n",
    "# Content of 'data.txt':\n",
    "# 1,2,3\n",
    "# 4,5,6\n",
    "# 7,8,9\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.readlines()\n",
    "    data = [line.strip().split(',') for line in data]\n",
    "    return data\n",
    "\n",
    "filename = 'data.txt'\n",
    "data = read_file(filename)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Scraping the Web        \n",
    "Example: Web Scraping with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to scrape\n",
    "url = 'http://example.com'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract and print the title of the webpage\n",
    "title = soup.title.string\n",
    "print('Title:', title)\n",
    "\n",
    "# Extract and print all paragraph texts\n",
    "paragraphs = soup.find_all('p')\n",
    "for p in paragraphs:\n",
    "    print(p.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Using APIs      \n",
    "Example: Using the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Replace with your own credentials\n",
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_token = 'your_access_token'\n",
    "access_token_secret = 'your_access_token_secret'\n",
    "\n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Get the User object for twitter\n",
    "user = api.get_user(screen_name='twitter')\n",
    "\n",
    "print('User details:')\n",
    "print('Name:', user.name)\n",
    "print('Screen name:', user.screen_name)\n",
    "print('Location:', user.location)\n",
    "print('Description:', user.description)\n",
    "\n",
    "print('Last 10 Followers:')\n",
    "for follower in user.followers(count=10):\n",
    "    print(follower.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Working with Data       \n",
    "Exploring Your Data     \n",
    "\n",
    "Example: Basic Data Exploration with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 4, 3, 2, 1],\n",
    "    'C': [2, 3, 4, 5, 6]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(df.info())\n",
    "\n",
    "# Display basic statistics of the DataFrame\n",
    "print(df.describe())\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Display the correlation matrix of the DataFrame\n",
    "print(df.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Using NamedTuples       \n",
    "Example: Creating and Using NamedTuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Define a namedtuple\n",
    "Point = namedtuple('Point', ['x', 'y'])\n",
    "\n",
    "# Create an instance of the namedtuple\n",
    "p = Point(1, 2)\n",
    "\n",
    "# Access elements by name\n",
    "print('x:', p.x)\n",
    "print('y:', p.y)\n",
    "\n",
    "# Access elements by index\n",
    "print('x:', p[0])\n",
    "print('y:', p[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Dataclasses     \n",
    "Example: Creating and Using Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "# Create an instance of the dataclass\n",
    "p = Point(1, 2)\n",
    "\n",
    "# Access elements\n",
    "print('x:', p.x)\n",
    "print('y:', p.y)\n",
    "\n",
    "# Print the dataclass instance\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Cleaning and Munging        \n",
    "Example: Data Cleaning with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = {\n",
    "    'A': [1, 2, None, 4, 5],\n",
    "    'B': [None, 4, 3, 2, 1],\n",
    "    'C': [2, None, 4, None, 6]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print('Original DataFrame:')\n",
    "print(df)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "print('\\nDataFrame after dropping rows with missing values:')\n",
    "print(df_dropped)\n",
    "\n",
    "# Fill missing values with a specified value\n",
    "df_filled = df.fillna(0)\n",
    "print('\\nDataFrame after filling missing values with 0:')\n",
    "print(df_filled)\n",
    "\n",
    "# Fill missing values with the mean of the column\n",
    "df_filled_mean = df.fillna(df.mean())\n",
    "print('\\nDataFrame after filling missing values with column mean:')\n",
    "print(df_filled_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Manipulating Data       \n",
    "Example: Data Manipulation with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 4, 3, 2, 1],\n",
    "    'C': [2, 3, 4, 5, 6]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Adding a new column\n",
    "df['D'] = df['A'] + df['B']\n",
    "print('DataFrame after adding column D:')\n",
    "print(df)\n",
    "\n",
    "# Renaming columns\n",
    "df.rename(columns={'A': 'Alpha', 'B': 'Beta'}, inplace=True)\n",
    "print('\\nDataFrame after renaming columns:')\n",
    "print(df)\n",
    "\n",
    "# Filtering rows\n",
    "filtered_df = df[df['Alpha'] > 2]\n",
    "print('\\nFiltered DataFrame where Alpha > 2:')\n",
    "print(filtered_df)\n",
    "\n",
    "# Grouping and aggregating data\n",
    "grouped_df = df.groupby('C').mean()\n",
    "print('\\nGrouped and aggregated DataFrame (mean of each group in column C):')\n",
    "print(grouped_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Rescaling       \n",
    "Example: Data Rescaling with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 4, 3, 2, 1],\n",
    "    'C': [2, 3, 4, 5, 6]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Min-Max Scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_min_max_scaled = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
    "print('DataFrame after Min-Max Scaling:')\n",
    "print(df_min_max_scaled)\n",
    "\n",
    "# Standard Scaling (Z-score normalization)\n",
    "standard_scaler = StandardScaler()\n",
    "df_standard_scaled = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)\n",
    "print('\\nDataFrame after Standard Scaling:')\n",
    "print(df_standard_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. An Aside: tqdm      \n",
    "Example: Progress Bar with tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Simulate a process that takes time using tqdm\n",
    "for i in tqdm(range(10)):\n",
    "    time.sleep(0.5)  # Simulate work by sleeping for 0.5 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Dimensionality Reduction        \n",
    "Example: PCA with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [5, 4, 3, 2, 1],\n",
    "    'C': [2, 3, 4, 5, 6]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(df)\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plot the PCA result\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_pca['PC1'], df_pca['PC2'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Sample Data')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
